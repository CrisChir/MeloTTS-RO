{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f55de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from melo.text import english, malay\n",
    "from melo.text.english_bert import get_bert_feature\n",
    "from melo.text.malay_bert import get_bert_feature as get_malay_bert_feature\n",
    "from melo.text.japanese import distribute_phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f81dc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_', 'hh', 'ah', 'l', 'ow', 'm', 'ay', 'n', 'ey', 'm', '.', '_'],\n",
       " [0, 0, 1, 0, 2, 0, 2, 0, 2, 0, 0, 0],\n",
       " [1, 4, 2, 3, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'hello my name.'\n",
    "text = english.text_normalize(text)\n",
    "phones, tones, word2ph = english.g2p(text)\n",
    "phones, tones, word2ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cbf0196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = get_bert_feature(text, word2ph)\n",
    "bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4bb508f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32458eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_',\n",
       "  'h',\n",
       "  'ËˆÉ›',\n",
       "  'l',\n",
       "  'o',\n",
       "  'n',\n",
       "  'Ëˆa',\n",
       "  'm',\n",
       "  'É™',\n",
       "  's',\n",
       "  'Ëˆa',\n",
       "  'j',\n",
       "  'É™',\n",
       "  '.',\n",
       "  '_'],\n",
       " [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [1, 5, 5, 5, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'hello nama saya.'\n",
    "text = malay.text_normalize(text)\n",
    "phones, tones, word2ph = malay.g2p(text)\n",
    "phones, tones, word2ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65740633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130d7ac92ee840b0b9ffcc2287b23c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/697 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3655aa6af54abe8995f449101b0fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 18])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = get_malay_bert_feature(text, word2ph)\n",
    "bert.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
